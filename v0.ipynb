{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Hrk-FtTB0XUUzOmR0qLMuu1sDlM2Dpr4",
      "authorship_tag": "ABX9TyNMaMsk26Q7OLCPJaquY9VU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alisa7979/20252R0136DATA30400/blob/main/2023320036_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT LIBRARIES & DATA PATH"
      ],
      "metadata": {
        "id": "J6q-Y8jbZXYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import drive\n",
        "\n",
        "# configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "drive.mount('/content/drive')\n",
        "ROOT = Path(\"/content/drive/MyDrive/Amazon_products\")\n",
        "TRAIN_CORPUS = ROOT / \"train\" / \"train_corpus.txt\"\n",
        "TEST_CORPUS = ROOT / \"test\" / \"test_corpus.txt\"\n",
        "KEYWORDS_PATH = ROOT / \"class_related_keywords.txt\"\n",
        "CLASSES_PATH = ROOT / \"classes.txt\"\n",
        "\n",
        "\n",
        "# set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ],
      "metadata": {
        "id": "AR2SVI2YZfM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca43e4f9-710f-4c3f-8735-5b27d0bca090"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load class\n",
        "cid2name = {}\n",
        "with open(CLASSES_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split('\\t')                            # parts = ['0', 'grocery_gourmet_food']\n",
        "        if len(parts) >= 2:\n",
        "            cid2name[int(parts[0])] = parts[1]                      # cid2name[0] = 'grocery_gourmet_food'\n",
        "\n",
        "# dictionary cid2name = {0: 'grocery_gourmet_food', 1: 'meat_poultry', ...}\n",
        "\n",
        "# dictionary cid2text = cid2name\n",
        "cid2text = {cid: name for cid, name in cid2name.items()}\n",
        "\n",
        "with open(KEYWORDS_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if ':' in line:\n",
        "            name_part, keyword_part = line.strip().split(':', 1)    # name part = 'grocery_gourmet_food', keyword_part = 'snacks,condiments,beverages...'\n",
        "            class_name = name_part.strip()                          # class_name = 'grocery_gourmet_food' for safety\n",
        "            # find ID for this name\n",
        "            found_id = None\n",
        "            for cid, cname in cid2name.items():                     # cid = 0, cname = 'grocery_gourmet_food'\n",
        "                if cname == class_name:\n",
        "                    found_id = cid                                  # found_id = 0\n",
        "                    break\n",
        "\n",
        "            if found_id is not None:\n",
        "                # append keywords to the class description\n",
        "                # replace commas with spaces for TF-IDF\n",
        "                clean_keyword = keyword_part.replace(',', ' ')      # clean_keyword = 'snacks condiments beverages ...'\n",
        "                cid2text[found_id] += \" \" + clean_keyword           # cid2text = {0: 'grocery_gourmet_food snacks condiments beverages ...', 1: 'meat_poultry butcher cuts...'}\n",
        "\n",
        "# sort classes for safery, get list of classes + keywords\n",
        "sorted_cids = sorted(cid2text.keys())                               # sorted_cids = [0, 1, 2, ..., 530]\n",
        "class_texts = [cid2text[cid] for cid in sorted_cids]                # class_texts = ['grocery_gourmet_food snacks condiments beverages ...', 'meat_poultry butcher cuts...', ...]\n",
        "\n",
        "print(f\"Prepared {len(class_texts)} Class Prototypes.\")\n",
        "\n",
        "# load train data\n",
        "train_pids, train_texts = [], []\n",
        "with open(TRAIN_CORPUS, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split('\\t')                            # parts = ['0', 'omron hem 790it automatic...']\n",
        "        if len(parts) >= 2:\n",
        "            train_pids.append(parts[0])                             # train_pids = ['0', '1', '2',...]\n",
        "            train_texts.append(parts[-1])                           # train_texts = ['omron hem 790it automatic...', 'natural factors whey factors...', ]\n",
        "print('Loaded Training Corpus.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX6wstfmJGz0",
        "outputId": "5845e1bb-95a0-4e2a-c178-42797601dad7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 531 Class Prototypes.\n",
            "Loaded Training Corpus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KAGGLE SUBMISSION"
      ],
      "metadata": {
        "id": "rzUBBVxpZunm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------\n",
        "# Dummy baseline for Kaggle submission\n",
        "# Generates random multi-label predictions\n",
        "# ------------------------\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Paths ---\n",
        "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
        "\n",
        "# --- Constants ---\n",
        "NUM_CLASSES = 531  # total number of classes (0–530)\n",
        "MIN_LABELS = 1     # minimum number of labels per sample\n",
        "MAX_LABELS = 3     # maximum number of labels per sample\n",
        "\n",
        "# --- Load test corpus ---\n",
        "def load_corpus(path):\n",
        "    \"\"\"Load test corpus into {pid: text} dictionary.\"\"\"\n",
        "    pid2text = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\", 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                pid2text[pid] = text\n",
        "    return pid2text\n",
        "\n",
        "pid2text_test = load_corpus(TEST_CORPUS)\n",
        "pid_list_test = list(pid2text_test.keys())\n",
        "\n",
        "# --- Generate random predictions ---\n",
        "all_pids, all_labels = [], []\n",
        "for pid in tqdm(pid_list_test, desc=\"Generating dummy predictions\"):\n",
        "    n_labels = random.randint(MIN_LABELS, MAX_LABELS)\n",
        "    labels = random.sample(range(NUM_CLASSES), n_labels)\n",
        "    labels = sorted(labels)\n",
        "    all_pids.append(pid)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "# --- Save submission file ---\n",
        "with open(\"submission.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # CHANGE 1: Use 'id' and 'label' as headers\n",
        "    writer.writerow([\"id\", \"label\"])\n",
        "\n",
        "    for pid, labels in zip(all_pids, all_labels):\n",
        "        # The logic here is already correct; it will auto-quote the label string\n",
        "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
        "\n",
        "print(f\"Dummy submission file saved to: {SUBMISSION_PATH}\")\n",
        "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqAPn4u7YpiK",
        "outputId": "cfd64fa1-d767-4a59-fa40-06315582edb4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating dummy predictions: 100%|██████████| 19658/19658 [00:00<00:00, 238309.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy submission file saved to: submission.csv\n",
            "Total samples: 19658, Classes per sample: 1-3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
